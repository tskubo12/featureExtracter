{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数の二次元のリストの重複する要素の削除をする\n",
    "def getOnlyWords(targetList):\n",
    "    result = []\n",
    "    for val in targetList:\n",
    "        if val not in result:\n",
    "            result.append(val)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数wordがtargetWordList内にいくつ存在しているか\n",
    "def wordCounting(word,targetWordList):\n",
    "    counter = 0\n",
    "    for targetWord in targetWordList:\n",
    "        if(word == targetWord):\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePickle(obj,filePath):\n",
    "    fileName = os.path.basename(filePath)\n",
    "    try:\n",
    "        with open(filePath,'wb') as f : \n",
    "            pickle.dump(obj,f)\n",
    "        print('writing {} success'.format(fileName))\n",
    "    except:\n",
    "        print('failed writing {}'.format(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWordListFromSections(mnemonicSections):\n",
    "    mnemonicSectionsNames = mnemonicSections.keys()\n",
    "    uniqueWordList = []\n",
    "    \n",
    "    for sectionName in mnemonicSectionsNames:\n",
    "        if(isSectionTextOrItext(sectionName)):\n",
    "            ngramDivided = getBigramDivide(mnemonicSections[sectionName])\n",
    "            print(len(ngramDivided))\n",
    "            uniqueWordList.extend(getOnlyWords(ngramDivided))\n",
    "            uniqueWordList = getOnlyWords(uniqueWordList)\n",
    "            print(len(uniqueWordList))\n",
    "\n",
    "    return uniqueWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニーモニックの配列をBigram分割した二次元配列を返す\n",
    "def getBigramDivide(mnemonicList):\n",
    "    n = 2 \n",
    "    bigram = []\n",
    "    for mindex in range(len(mnemonicList) - n + 1):\n",
    "        ngramWord = mnemonicList[mindex:mindex + n]\n",
    "        bigram.append(ngramWord)\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSectionTextOrItext(sectionName):\n",
    "    antiPattern = '<.text>_0'\n",
    "    hopePattern_text = '<.text>_.'\n",
    "    hopePattern_itext = '<.itext>_.'\n",
    "    retVal = False\n",
    "    \n",
    "    isMatch_Text = re.match(hopePattern_text,sectionName)\n",
    "    isMatch_Itext = re.match(hopePattern_itext,sectionName)\n",
    "    if(antiPattern == sectionName):\n",
    "        retVal = False\n",
    "    elif(isMatch_Text or isMatch_Itext):\n",
    "        retVal = True\n",
    "    \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileLoader(filepath):\n",
    "    with open(filepath) as f:\n",
    "        dictDataFromJson = json.load(f)\n",
    "    \n",
    "    return dictDataFromJson['mnemonics']\n",
    "#     print(os.path.basename(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPe32(fileTypeStr):\n",
    "    ifInThisWord = r'PE32.*'\n",
    "    \n",
    "    if re.findall(ifInThisWord,fileTypeStr):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process : 1/9218\n",
      "process : 2/9218\n",
      "828\n",
      "131\n",
      "process : 3/9218\n",
      "1328496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-9b7d1afd9a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mwritePicke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallWordDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../allWordDict.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-9b7d1afd9a3a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmnemonicSections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mallWordDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetUniqueWordListFromSections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnemonicSections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mallWordDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOnlyWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallWordDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-9d607a91e55c>\u001b[0m in \u001b[0;36mgetUniqueWordListFromSections\u001b[0;34m(mnemonicSections)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mngramDivided\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetBigramDivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnemonicSections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msectionName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngramDivided\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0muniqueWordList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetOnlyWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngramDivided\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0muniqueWordList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOnlyWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueWordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueWordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-8c53cf465239>\u001b[0m in \u001b[0;36mgetOnlyWords\u001b[0;34m(targetList)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargetList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    targetDirPath = '/media/sf_VirtualBox_share/results_featureExtracter/assemblyTxt/'\n",
    "    for (dirpath,dirnames,filenames) in os.walk(targetDirPath):\n",
    "        idx  = 1\n",
    "        fileCounter = len(filenames)\n",
    "        allWordDict = []\n",
    "        for fileName in filenames:\n",
    "            print('process : {0}/{1}'.format(idx,fileCounter))\n",
    "\n",
    "            mnemonicSections = fileLoader(os.path.join(dirpath,fileName)) \n",
    "\n",
    "            allWordDict.extend(getUniqueWordListFromSections(mnemonicSections))\n",
    "            idx += 1\n",
    "        allWordDict = getOnlyWords(allWordDict)\n",
    "        print(len(allWordDict))\n",
    "        print('Finish Process')\n",
    "        writePicke(allWordDict,'../allWordDict.pickle')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    for idx in range(len(gramLists)):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# ##############ここらへんから怪しい###############\n",
    "#             for idx in range(len(gramLists)):\n",
    "#                 gramLists[idx].extend(ret[idx])\n",
    "#                 gramLists[idx] = getOnlyWords(gramLists[idx])\n",
    "#                 print('gram {}'.format(idx))\n",
    "#     for idx in range(len(gramLists)):\n",
    "#         gramLists[idx] = getOnlyWords(gramLists[idx])\n",
    "#         print('{} gram list : {}'.format(idx+1,len(gramLists[idx])))\n",
    "#         writePickle(gramLists[idx], dirs[1] + 'gram_{}.pickle'.format(idx + 1))\n",
    "        \n",
    "#     for dirpath,dirnames,filenames in os.walk(dirs[0]):\n",
    "#         for filename in filenames:\n",
    "#             with open(dirpath + filename,'r') as f:\n",
    "#                 json_obj = json.load(f)\n",
    "#                 ngramListsRaw = getWords(json_obj)\n",
    "#                 for index , ngramRaw in enumerate(ngramListsRaw):\n",
    "#                     print('-------{} gram--------'.format(index + 1)) \n",
    "#                     for word in gramLists[index]:\n",
    "#                         count = wordCounting(word,ngramRaw)\n",
    "#                         tupleList[index].append((word,count))\n",
    "                    \n",
    "#                     writePickle(tupleList[index],dirs[index + 3] + os.path.splitext(filename)[0] + '.pickle')\n",
    "\n",
    "\n",
    "#             print('getWords ....')\n",
    "#             ret = getWords(assembly)\n",
    "#             print('complete : getwords')\n",
    "            \n",
    "#             print('extendWording, getOnlyWords ..............')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.4",
    "jupytext_version": "1.2.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
