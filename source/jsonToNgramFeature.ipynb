{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数の二次元のリストの重複する要素の削除をする\n",
    "def getOnlyWords(targetList):\n",
    "    result = []\n",
    "    index = 1\n",
    "    listLength = len(targetList)\n",
    "    for val in targetList:\n",
    "        if val not in result:\n",
    "            result.append(val)\n",
    "        if((index%10000) == 0):\n",
    "            print('{0}/{1}'.format(index,listLength))\n",
    "        index +=1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数wordがtargetWordList内にいくつ存在しているか\n",
    "def wordCounting(word,targetWordList):\n",
    "    counter = 0\n",
    "    for targetWord in targetWordList:\n",
    "        if(word == targetWord):\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePickle(obj,filePath):\n",
    "    fileName = os.path.basename(filePath)\n",
    "#     try:\n",
    "    with open(filePath,'wb') as f : \n",
    "        pickle.dump(obj,f)\n",
    "    print('writing {} success'.format(fileName))\n",
    "#     except:\n",
    "#         print('failed writing {}'.format(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWordListFromSections(mnemonicSections):\n",
    "    mnemonicSectionsNames = mnemonicSections.keys()\n",
    "    uniqueWordList = []\n",
    "    \n",
    "    for sectionName in mnemonicSectionsNames:\n",
    "        if(isSectionTextOrItext(sectionName)):\n",
    "            ngramDivided = getBigramDivide(mnemonicSections[sectionName])\n",
    "            print(len(ngramDivided))\n",
    "            uniqueWordList.extend(getOnlyWords(ngramDivided))\n",
    "            uniqueWordList = getOnlyWords(uniqueWordList)\n",
    "            print(len(uniqueWordList))\n",
    "\n",
    "    return uniqueWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニーモニックの配列をBigram分割した二次元配列を返す\n",
    "def getBigramDivide(mnemonicList):\n",
    "    n = 2 \n",
    "    bigram = []\n",
    "    for mindex in range(len(mnemonicList) - n + 1):\n",
    "        ngramWord = mnemonicList[mindex:mindex + n]\n",
    "        bigram.append(ngramWord)\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSectionTextOrItext(sectionName):\n",
    "    antiPattern = '<.text>_0'\n",
    "    hopePattern_text = '<.text>_.'\n",
    "    hopePattern_itext = '<.itext>_.'\n",
    "    retVal = False\n",
    "    \n",
    "    isMatch_Text = re.match(hopePattern_text,sectionName)\n",
    "    isMatch_Itext = re.match(hopePattern_itext,sectionName)\n",
    "    if(antiPattern == sectionName):\n",
    "        retVal = False\n",
    "    elif(isMatch_Text or isMatch_Itext):\n",
    "        retVal = True\n",
    "    \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileLoader(filepath):\n",
    "    with open(filepath) as f:\n",
    "        dictDataFromJson = json.load(f)\n",
    "    \n",
    "    return dictDataFromJson['mnemonics']\n",
    "#     print(os.path.basename(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPe32(fileTypeStr):\n",
    "    ifInThisWord = r'PE32.*'\n",
    "    \n",
    "    if re.findall(ifInThisWord,fileTypeStr):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDirPath=r'C:\\Users\\HIROKI\\Downloads\\results_20191205\\results\\assemblyTxt'\n",
    "for (dirpath,dirnames,filenames) in os.walk(targetDirPath):\n",
    "    idx  = 1\n",
    "    fileCounter = len(filenames)\n",
    "    allWordDict = []\n",
    "    for fileName in filenames:\n",
    "        print('process : {0}/{1}'.format(idx,fileCounter))\n",
    "\n",
    "        mnemonicSections = fileLoader(os.path.join(dirpath,fileName)) \n",
    "\n",
    "        allWordDict.extend(getUniqueWordListFromSections(mnemonicSections))\n",
    "        idx += 1\n",
    "\n",
    "        \n",
    "    writePickle(allWordDict,r'..\\allWordDict_before.pickle')\n",
    "    allWordDict = getOnlyWords(allWordDict)\n",
    "    print(len(allWordDict))\n",
    "    print('Finish Process')\n",
    "    writePickle(allWordDict,r'..\\allWordDict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    for idx in range(len(gramLists)):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# ##############ここらへんから怪しい###############\n",
    "#             for idx in range(len(gramLists)):\n",
    "#                 gramLists[idx].extend(ret[idx])\n",
    "#                 gramLists[idx] = getOnlyWords(gramLists[idx])\n",
    "#                 print('gram {}'.format(idx))\n",
    "#     for idx in range(len(gramLists)):\n",
    "#         gramLists[idx] = getOnlyWords(gramLists[idx])\n",
    "#         print('{} gram list : {}'.format(idx+1,len(gramLists[idx])))\n",
    "#         writePickle(gramLists[idx], dirs[1] + 'gram_{}.pickle'.format(idx + 1))\n",
    "        \n",
    "#     for dirpath,dirnames,filenames in os.walk(dirs[0]):\n",
    "#         for filename in filenames:\n",
    "#             with open(dirpath + filename,'r') as f:\n",
    "#                 json_obj = json.load(f)\n",
    "#                 ngramListsRaw = getWords(json_obj)\n",
    "#                 for index , ngramRaw in enumerate(ngramListsRaw):\n",
    "#                     print('-------{} gram--------'.format(index + 1)) \n",
    "#                     for word in gramLists[index]:\n",
    "#                         count = wordCounting(word,ngramRaw)\n",
    "#                         tupleList[index].append((word,count))\n",
    "                    \n",
    "#                     writePickle(tupleList[index],dirs[index + 3] + os.path.splitext(filename)[0] + '.pickle')\n",
    "\n",
    "\n",
    "#             print('getWords ....')\n",
    "#             ret = getWords(assembly)\n",
    "#             print('complete : getwords')\n",
    "            \n",
    "#             print('extendWording, getOnlyWords ..............')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.4",
    "jupytext_version": "1.2.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
